{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/rokickik/dev/ngffbrowse/data/test.h5j',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/N2_352-1.n5',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/Untitled1.ipynb',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/.DS_Store',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/state.json',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/out2.json',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/mobie',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/.ipynb_checkpoints',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/bioformats2raw_versions.yml',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/n5_ANM525849',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/out.json',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/N2_352-1.zarr',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/serve.sh',\n",
       " '/Users/rokickik/dev/ngffbrowse/data/single.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fsspec\n",
    "from fsspec.implementations.local import LocalFileSystem\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_fs(url):\n",
    "    pu = urlparse(url)\n",
    "    if pu.scheme in ['http','https'] and pu.netloc.endswith('.s3.amazonaws.com'):\n",
    "        # Convert S3 HTTP URLs (which do not support list operations) back to S3 REST API\n",
    "        fs = fsspec.filesystem('s3')\n",
    "        p = pu.netloc.split('.')[0] + pu.path\n",
    "    else:\n",
    "        fs = fsspec.filesystem(pu.scheme)\n",
    "        p = pu.netloc + pu.path\n",
    "    return fs, p\n",
    "\n",
    "url1 = \"file://data\"\n",
    "fs1, fs1root = get_fs(url1)\n",
    "fs1.ls(fs1root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Key': 'janelia-flylight-imagery-dev/Fly-eFISH/',\n",
       "  'LastModified': datetime.datetime(2024, 2, 23, 15, 40, 52, tzinfo=tzutc()),\n",
       "  'ETag': '\"d41d8cd98f00b204e9800998ecf8427e\"',\n",
       "  'Size': 0,\n",
       "  'StorageClass': 'STANDARD',\n",
       "  'type': 'file',\n",
       "  'size': 0,\n",
       "  'name': 'janelia-flylight-imagery-dev/Fly-eFISH/'},\n",
       " {'Key': 'janelia-flylight-imagery-dev/Fly-eFISH/NP01_1_1_SS00790.json',\n",
       "  'LastModified': datetime.datetime(2024, 2, 29, 20, 59, 29, tzinfo=tzutc()),\n",
       "  'ETag': '\"eb8fd5db7b1bfdba7c3a076f23ed9643\"',\n",
       "  'Size': 3459,\n",
       "  'StorageClass': 'STANDARD',\n",
       "  'type': 'file',\n",
       "  'size': 3459,\n",
       "  'name': 'janelia-flylight-imagery-dev/Fly-eFISH/NP01_1_1_SS00790.json'},\n",
       " {'Key': 'janelia-flylight-imagery-dev/Fly-eFISH/state.json',\n",
       "  'LastModified': datetime.datetime(2024, 2, 28, 20, 32, 8, tzinfo=tzutc()),\n",
       "  'ETag': '\"7db26713fa2b049c3508145aed6dfec4\"',\n",
       "  'Size': 1101,\n",
       "  'StorageClass': 'STANDARD',\n",
       "  'type': 'file',\n",
       "  'size': 1101,\n",
       "  'name': 'janelia-flylight-imagery-dev/Fly-eFISH/state.json'},\n",
       " {'Key': 'janelia-flylight-imagery-dev/Fly-eFISH/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.chunked.zarr',\n",
       "  'Size': 0,\n",
       "  'StorageClass': 'DIRECTORY',\n",
       "  'type': 'directory',\n",
       "  'size': 0,\n",
       "  'name': 'janelia-flylight-imagery-dev/Fly-eFISH/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.chunked.zarr'},\n",
       " {'Key': 'janelia-flylight-imagery-dev/Fly-eFISH/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr',\n",
       "  'Size': 0,\n",
       "  'StorageClass': 'DIRECTORY',\n",
       "  'type': 'directory',\n",
       "  'size': 0,\n",
       "  'name': 'janelia-flylight-imagery-dev/Fly-eFISH/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url2 = \"s3://janelia-flylight-imagery-dev/Fly-eFISH\"\n",
    "fs2, fs2root = get_fs(url2)\n",
    "fs2.ls(fs2root, detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url2 = \"https://janelia-flylight-imagery-dev.s3.amazonaws.com/Fly-eFISH\"\n",
    "fs2, fs2root = get_fs(url2)\n",
    "len(fs2.ls(fs2root, detail=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Key': 'janelia-flylight-imagery-dev/Fly-eFISH/state.json',\n",
       " 'LastModified': datetime.datetime(2024, 2, 28, 20, 32, 8, tzinfo=tzutc()),\n",
       " 'ETag': '\"7db26713fa2b049c3508145aed6dfec4\"',\n",
       " 'Size': 1101,\n",
       " 'StorageClass': 'STANDARD',\n",
       " 'type': 'file',\n",
       " 'size': 1101,\n",
       " 'name': 'janelia-flylight-imagery-dev/Fly-eFISH/state.json'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url2 = \"https://janelia-flylight-imagery-dev.s3.amazonaws.com/Fly-eFISH/state.json\"\n",
    "fs2, fs2root = get_fs(url2)\n",
    "fs2.info(fs2root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file',\n",
       " 'memory',\n",
       " 'dropbox',\n",
       " 'http',\n",
       " 'https',\n",
       " 'zip',\n",
       " 'tar',\n",
       " 'gcs',\n",
       " 'gs',\n",
       " 'gdrive',\n",
       " 'sftp',\n",
       " 'ssh',\n",
       " 'ftp',\n",
       " 'hdfs',\n",
       " 'arrow_hdfs',\n",
       " 'webhdfs',\n",
       " 's3',\n",
       " 's3a',\n",
       " 'wandb',\n",
       " 'oci',\n",
       " 'asynclocal',\n",
       " 'adl',\n",
       " 'abfs',\n",
       " 'az',\n",
       " 'cached',\n",
       " 'blockcache',\n",
       " 'filecache',\n",
       " 'simplecache',\n",
       " 'dask',\n",
       " 'dbfs',\n",
       " 'github',\n",
       " 'git',\n",
       " 'smb',\n",
       " 'jupyter',\n",
       " 'jlab',\n",
       " 'libarchive',\n",
       " 'reference',\n",
       " 'generic',\n",
       " 'oss',\n",
       " 'webdav',\n",
       " 'dvc',\n",
       " 'hf',\n",
       " 'root',\n",
       " 'dir',\n",
       " 'box']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://filesystem-spec.readthedocs.io/en/latest/api.html#implementations\n",
    "fsspec.available_protocols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Access S3 as if it were a file system.\n",
      "\n",
      "    This exposes a filesystem-like API (ls, cp, open, etc.) on top of S3\n",
      "    storage.\n",
      "\n",
      "    Provide credentials either explicitly (``key=``, ``secret=``) or depend\n",
      "    on boto's credential methods. See botocore documentation for more\n",
      "    information. If no credentials are available, use ``anon=True``.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    anon : bool (False)\n",
      "        Whether to use anonymous connection (public buckets only). If False,\n",
      "        uses the key/secret given, or boto's credential resolver (client_kwargs,\n",
      "        environment, variables, config files, EC2 IAM server, in that order)\n",
      "    endpoint_url : string (None)\n",
      "        Use this endpoint_url, if specified. Needed for connecting to non-AWS\n",
      "        S3 buckets. Takes precedence over `endpoint_url` in client_kwargs.\n",
      "    key : string (None)\n",
      "        If not anonymous, use this access key ID, if specified. Takes precedence\n",
      "        over `aws_access_key_id` in client_kwargs.\n",
      "    secret : string (None)\n",
      "        If not anonymous, use this secret access key, if specified. Takes\n",
      "        precedence over `aws_secret_access_key` in client_kwargs.\n",
      "    token : string (None)\n",
      "        If not anonymous, use this security token, if specified\n",
      "    use_ssl : bool (True)\n",
      "        Whether to use SSL in connections to S3; may be faster without, but\n",
      "        insecure. If ``use_ssl`` is also set in ``client_kwargs``,\n",
      "        the value set in ``client_kwargs`` will take priority.\n",
      "    s3_additional_kwargs : dict of parameters that are used when calling s3 api\n",
      "        methods. Typically used for things like \"ServerSideEncryption\".\n",
      "    client_kwargs : dict of parameters for the botocore client\n",
      "    requester_pays : bool (False)\n",
      "        If RequesterPays buckets are supported.\n",
      "    default_block_size: int (None)\n",
      "        If given, the default block size value used for ``open()``, if no\n",
      "        specific value is given at all time. The built-in default is 5MB.\n",
      "    default_fill_cache : Bool (True)\n",
      "        Whether to use cache filling with open by default. Refer to\n",
      "        ``S3File.open``.\n",
      "    default_cache_type : string (\"readahead\")\n",
      "        If given, the default cache_type value used for ``open()``. Set to \"none\"\n",
      "        if no caching is desired. See fsspec's documentation for other available\n",
      "        cache_type values. Default cache_type is \"readahead\".\n",
      "    version_aware : bool (False)\n",
      "        Whether to support bucket versioning.  If enable this will require the\n",
      "        user to have the necessary IAM permissions for dealing with versioned\n",
      "        objects. Note that in the event that you only need to work with the\n",
      "        latest version of objects in a versioned bucket, and do not need the\n",
      "        VersionId for those objects, you should set ``version_aware`` to False\n",
      "        for performance reasons. When set to True, filesystem instances will\n",
      "        use the S3 ListObjectVersions API call to list directory contents,\n",
      "        which requires listing all historical object versions.\n",
      "    cache_regions : bool (False)\n",
      "        Whether to cache bucket regions or not. Whenever a new bucket is used,\n",
      "        it will first find out which region it belongs and then use the client\n",
      "        for that region.\n",
      "    asynchronous :  bool (False)\n",
      "        Whether this instance is to be used from inside coroutines.\n",
      "    config_kwargs : dict of parameters passed to ``botocore.client.Config``\n",
      "    kwargs : other parameters for core session.\n",
      "    session : aiobotocore AioSession object to be used for all connections.\n",
      "         This session will be used inplace of creating a new session inside S3FileSystem.\n",
      "         For example: aiobotocore.session.AioSession(profile='test_user')\n",
      "\n",
      "    The following parameters are passed on to fsspec:\n",
      "\n",
      "    skip_instance_cache: to control reuse of instances\n",
      "    use_listings_cache, listings_expiry_time, max_paths: to control reuse of directory listings\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> s3 = S3FileSystem(anon=False)  # doctest: +SKIP\n",
      "    >>> s3.ls('my-bucket/')  # doctest: +SKIP\n",
      "    ['my-file.txt']\n",
      "\n",
      "    >>> with s3.open('my-bucket/my-file.txt', mode='rb') as f:  # doctest: +SKIP\n",
      "    ...     print(f.read())  # doctest: +SKIP\n",
      "    b'Hello, world!'\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(fsspec.get_filesystem_class(\"s3\").__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  N5 N2_352-1.n5\n",
      "        N5 ARRAY N2_352-1.n5/image/c0/s0\n",
      "  bioformats2raw series NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr\n",
      "    ZARR MULTISCALE NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0\n",
      "      ZARR ARRAY NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0/0\n",
      "      ZARR ARRAY NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0/1\n",
      "      ZARR ARRAY NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0/3\n",
      "      ZARR ARRAY NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0/2\n",
      "  N5 n5_ANM525849\n",
      "    N5 MULTISCALE n5_ANM525849/c0\n",
      "      N5 ARRAY n5_ANM525849/c0/s2\n",
      "      N5 ARRAY n5_ANM525849/c0/s1\n",
      "      N5 ARRAY n5_ANM525849/c0/s0\n",
      "    N5 MULTISCALE n5_ANM525849/c1\n",
      "      N5 ARRAY n5_ANM525849/c1/s2\n",
      "      N5 ARRAY n5_ANM525849/c1/s1\n",
      "      N5 ARRAY n5_ANM525849/c1/s0\n",
      "    ZARR MULTISCALE N2_352-1.zarr/image\n",
      "      ZARR ARRAY N2_352-1.zarr/image/s0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def find_ngff(fs, root, path, depth=0):\n",
    "    if depth>10: return []\n",
    "    indent = depth * '  '\n",
    "    #name = os.path.basename(path)\n",
    "    name = os.path.relpath(path, start=root)\n",
    "    #print(indent+path+\" \"+name)\n",
    "    children = fs.ls(path, detail=True)\n",
    "    child_names = [os.path.basename(c['name']) for c in children]\n",
    "    if '.zattrs' in child_names:\n",
    "        with fsspec.open(path+'/.zattrs') as f:\n",
    "            attrs = json.load(f)\n",
    "            if 'multiscales' in attrs:\n",
    "                print(indent+'ZARR MULTISCALE '+name)\n",
    "            if 'bioformats2raw.layout' in attrs:\n",
    "                print(indent+'bioformats2raw series '+name)\n",
    "\n",
    "    if '.zarray' in child_names:\n",
    "        print(indent+'ZARR ARRAY '+name)\n",
    "        return\n",
    "    \n",
    "    if 'attributes.json' in child_names:\n",
    "        with fsspec.open(path+'/attributes.json') as f:\n",
    "            attrs = json.load(f)\n",
    "            if 'scales' in attrs:\n",
    "                print(indent+'N5 MULTISCALE '+name)\n",
    "            elif 'dimensions' in attrs:\n",
    "                print(indent+'N5 ARRAY '+name)\n",
    "                return\n",
    "            elif 'n5' in attrs:\n",
    "                #re.match(\"^c\\d+$\", \"cf\")\n",
    "\n",
    "                print(indent+'N5 '+name)\n",
    "\n",
    "    for d in [i['name'] for i in children if i['type']=='directory']:\n",
    "        find_ngff(fs, root, d, depth+1)\n",
    "        \n",
    "find_ngff(fs1, fs1root, fs1root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional, Any, Dict, Literal\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bioformats2raw series NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr\n",
      "    ZARR MULTISCALE NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0\n",
      "      ZARR ARRAY NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0/0\n",
      "      ZARR ARRAY NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0/1\n",
      "      ZARR ARRAY NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0/3\n",
      "      ZARR ARRAY NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0/2\n",
      "    ZARR MULTISCALE N2_352-1.zarr/image\n",
      "      ZARR ARRAY N2_352-1.zarr/image/s0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def find_ome_zarrs(fs, root, path, depth=0):\n",
    "    if depth>10: return []\n",
    "    indent = depth * '  '\n",
    "    name = os.path.relpath(path, start=root)\n",
    "    children = fs.ls(path, detail=True)\n",
    "    child_names = [os.path.basename(c['name']) for c in children]\n",
    "    if '.zattrs' in child_names:\n",
    "        with fsspec.open(path+'/.zattrs') as f:\n",
    "            attrs = json.load(f)\n",
    "            if 'multiscales' in attrs:\n",
    "                print(indent+'ZARR MULTISCALE '+name)\n",
    "            if 'bioformats2raw.layout' in attrs:\n",
    "                print(indent+'bioformats2raw series '+name)\n",
    "\n",
    "    if '.zarray' in child_names:\n",
    "        print(indent+'ZARR ARRAY '+name)\n",
    "        return\n",
    "    \n",
    "\n",
    "    for d in [i['name'] for i in children if i['type']=='directory']:\n",
    "        find_ome_zarrs(fs, root, d, depth+1)\n",
    "        \n",
    "find_ome_zarrs(fs1, fs1root, fs1root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Key>Fly-eFISH/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.chunked.zarr/0/labels/.zattrs</Key><RequestId>JMPQ1066EVC7NQVY</RequestId><HostId>MYYlrY4+RSd8W5oLgypv4YP7T5G4V6nBoDL9sHFmIxdZVumSGudtAKW4COr18AFnbb1Fc5bBjVg36oDxHXqLCw==</HostId></Error>\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjects.html#API_ListObjects_ResponseSyntax\"\n",
    "\n",
    "\"Content-Type: binary/octet-stream\"\n",
    "\n",
    "\"\"\"\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Key>Fly-eFISH/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.chunked.zarr/0/labels/.zattrs</Key><RequestId>JMPQ1066EVC7NQVY</RequestId><HostId>MYYlrY4+RSd8W5oLgypv4YP7T5G4V6nBoDL9sHFmIxdZVumSGudtAKW4COr18AFnbb1Fc5bBjVg36oDxHXqLCw==</HostId></Error>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='01' path='./data/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0' axes='TCZYX' num_channels=4 num_timepoints=1 dimensions='1 ✕ 4 ✕ 376.00 μm ✕ 447.29 μm ✕ 447.29 μm' dimensions_voxels='1 ✕ 4 ✕ 752 ✕ 1920 ✕ 1920' chunk_size='1 ✕ 1 ✕ 1 ✕ 1920 ✕ 1920' voxel_sizes='1 ✕ 1 ✕ 0.50 μm ✕ 0.23 μm ✕ 0.23 μm' compression=\"Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0)\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Image(BaseModel):\n",
    "    model_config = ConfigDict(extra='forbid') \n",
    "    id: str = Field(title=\"Id\", description=\"Id for the data set container (unique within the parent folder)\")\n",
    "    path: str = Field(title=\"Path\", description=\"Path to the container, relative to the overall root\")\n",
    "    axes: str = Field(title=\"Axes\", description=\"Axes \")\n",
    "    num_channels: int = Field(title=\"Num Channels\", description=\"Number of channels in the image\")\n",
    "    num_timepoints: int = Field(title=\"Num Timepoints\", description=\"Number of timepoints in the image\")\n",
    "    dimensions: str = Field(title=\"Dimensions\", description=\"Size of the whole data set in nanometers\")\n",
    "    dimensions_voxels: str = Field(title=\"Dimensions (voxels)\", description=\"Size of the whole data set in voxels\")\n",
    "    chunk_size: str = Field(title=\"Chunk size\", description=\"Size of Zarr chunks\")\n",
    "    voxel_sizes: str = Field(title=\"Voxel Size\", description=\"Size of voxels in nanometers. XYZ ordering.\")\n",
    "    compression: str = Field(title=\"Compression\", description=\"Description of the compression used on the image data\")\n",
    "\n",
    "\n",
    "def encode_image(id, url, image_group):\n",
    "    multiscales = image_group.attrs['multiscales']\n",
    "    # TODO: what to do if there are multiple multiscales?\n",
    "    multiscale = multiscales[0]\n",
    "    axes = multiscale['axes']\n",
    "\n",
    "    # Use highest resolution \n",
    "    dataset = multiscale['datasets'][0]\n",
    "    array = image_group[image_group.name+'/'+dataset['path']]\n",
    "    \n",
    "    # TODO: shouldn't assume a single transform\n",
    "    scale = dataset['coordinateTransformations'][0]['scale']\n",
    "\n",
    "    axes_names = []\n",
    "    dimensions_voxels = []\n",
    "    voxel_sizes = []\n",
    "    dimensions = []\n",
    "    chunks = []\n",
    "    num_channels = 1\n",
    "    num_timepoints = 1\n",
    "    for i, axis in enumerate(axes):\n",
    "        axes_names.append(axis['name'].upper())\n",
    "        unit = ''\n",
    "        if axis['type']=='space':\n",
    "            unit = axis['unit']\n",
    "            if unit=='micrometer': unit = \" μm\"\n",
    "            if unit=='nanometer': unit = \" nm\"\n",
    "            voxel_sizes.append(\"%.2f%s\" % (round(scale[i],2), unit))\n",
    "            dimensions.append(\"%.2f%s\" % (round(array.shape[i] * scale[i],2), unit))\n",
    "        elif axis['type']=='channel':\n",
    "            num_channels = array.shape[i]\n",
    "            voxel_sizes.append(\"%i\" % scale[i])\n",
    "            dimensions.append(\"%i\" % (array.shape[i] * scale[i]))\n",
    "        elif axis['type']=='time':\n",
    "            num_timepoints = array.shape[i]\n",
    "            voxel_sizes.append(\"%i\" % scale[i])\n",
    "            dimensions.append(\"%i\" % (array.shape[i] * scale[i]))\n",
    "        dimensions_voxels.append(str(array.shape[i]))\n",
    "        chunks.append(\"%i\" % array.chunks[i])\n",
    "\n",
    "    return Image(\n",
    "        id = id,\n",
    "        path = url,\n",
    "        axes = ''.join(axes_names),\n",
    "        num_channels = num_channels,\n",
    "        num_timepoints = num_timepoints,\n",
    "        voxel_sizes = ' ✕ '.join(voxel_sizes),\n",
    "        dimensions = ' ✕ '.join(dimensions),\n",
    "        dimensions_voxels = ' ✕ '.join(dimensions_voxels),\n",
    "        chunk_size = ' ✕ '.join(chunks),\n",
    "        compression = str(array.compressor)\n",
    "    )\n",
    "\n",
    "import zarr\n",
    "url = \"./data/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr/0\"\n",
    "z = zarr.open(url, mode='r')\n",
    "\n",
    "image = encode_image('01', url, z)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images from ./data/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr\n",
      "Image(id='./data/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr', path='./data/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr', axes='TCZYX', num_channels=4, num_timepoints=1, dimensions='1 ✕ 4 ✕ 376.00 μm ✕ 447.29 μm ✕ 447.29 μm', dimensions_voxels='1 ✕ 4 ✕ 752 ✕ 1920 ✕ 1920', chunk_size='1 ✕ 1 ✕ 1 ✕ 1920 ✕ 1920', voxel_sizes='1 ✕ 1 ✕ 0.50 μm ✕ 0.23 μm ✕ 0.23 μm', compression=\"Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0)\")\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import zarr\n",
    "    \n",
    "def yield_images_nested(z):\n",
    "    # This only works with storage backends that support listing items like \n",
    "    # local disk and S3, but not HTTP for example.\n",
    "    for _,group in z.groups():\n",
    "        if 'multiscales' in group.attrs:\n",
    "            yield group\n",
    "        for image in yield_images_nested(group):\n",
    "            yield image\n",
    "\n",
    "def yield_images(url):\n",
    "    ''' Interrogates the OME-Zarr at the given URL and yields all of the 2-5D images within.\n",
    "    '''\n",
    "    z = zarr.open(url, mode='r')\n",
    "    # Based on https://ngff.openmicroscopy.org/latest/#bf2raw\n",
    "    if 'bioformats2raw.layout' in z.attrs and z.attrs['bioformats2raw.layout']==3:\n",
    "        if 'OME' in z:\n",
    "            series = z['OME'].attrs['series']\n",
    "            if len(series) == 1:\n",
    "                # We treat this as a single image for easier consumption\n",
    "                yield z[series[0]]\n",
    "            else:\n",
    "                # Spec: \"series\" MUST be a list of string objects, each of which is a path to an image group.\n",
    "                for image_id in series:\n",
    "                    yield z[image_id]\n",
    "        else:\n",
    "            # Spec: If the \"series\" attribute does not exist and no \"plate\" is present:\n",
    "            # - separate \"multiscales\" images MUST be stored in consecutively numbered groups starting from 0 (i.e. \"0/\", \"1/\", \"2/\", \"3/\", ...).\n",
    "            for i in itertools.count():\n",
    "                try:\n",
    "                    yield z[str(i)]\n",
    "                except:\n",
    "                    break\n",
    "    elif 'multiscales' in z.attrs:\n",
    "        yield z\n",
    "    else:\n",
    "        for image in yield_images_nested(z):\n",
    "            yield image\n",
    "\n",
    "def get_images(url):\n",
    "    print(f\"Getting images from {url}\")\n",
    "    for image_group in yield_images(url):\n",
    "        image = encode_image(url, url, image_group)\n",
    "        print(image.__repr__())\n",
    "\n",
    "\n",
    "url = \"./data/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.czi.zarr\"\n",
    "get_images(url)\n",
    "#%timeit get_images(url)\n",
    "#812 µs ± 11 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images from https://janelia-flylight-imagery-dev.s3.amazonaws.com/Fly-eFISH/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.chunked.zarr\n",
      "Image(id='https://janelia-flylight-imagery-dev.s3.amazonaws.com/Fly-eFISH/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.chunked.zarr', path='https://janelia-flylight-imagery-dev.s3.amazonaws.com/Fly-eFISH/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.chunked.zarr', axes='TCZYX', num_channels=4, num_timepoints=1, dimensions='1 ✕ 4 ✕ 376.00 μm ✕ 447.29 μm ✕ 447.29 μm', dimensions_voxels='1 ✕ 4 ✕ 752 ✕ 1920 ✕ 1920', chunk_size='1 ✕ 1 ✕ 128 ✕ 128 ✕ 128', voxel_sizes='1 ✕ 1 ✕ 0.50 μm ✕ 0.23 μm ✕ 0.23 μm', compression=\"Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0)\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://janelia-flylight-imagery-dev.s3.amazonaws.com/Fly-eFISH/NP01_1_1_SS00790_AstA546_CCHa1_647_1x_LOL.chunked.zarr\"\n",
    "get_images(url)\n",
    "#%timeit get_images(url)\n",
    "#365 ms ± 51.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = zarr.open(url, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
